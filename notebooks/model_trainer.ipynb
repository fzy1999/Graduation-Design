{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8966de37b103ead2",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook is used to train a model for the sketch detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlled-medline",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/bin:/usr/local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "/root/.local/bin:/root/miniconda3/bin:/usr/local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = os.environ['HOME'] + '/.local/bin:' + os.environ['PATH']\n",
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "planned-spelling",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strange-links",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f955c7b8-425b-4457-820c-945a3d130e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b6e6c85fe4dc13",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu118\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.8/site-packages (2.0.0+cu118)\n",
      "Requirement already satisfied: torchvision in /root/miniconda3/lib/python3.8/site-packages (0.15.1+cu118)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.8/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.8/site-packages (from torch) (3.10.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: lit in /root/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniconda3/lib/python3.8/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.8/site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.8/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests->torchvision) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests->torchvision) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-29u2pqd6\n",
      "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-29u2pqd6\n",
      "Requirement already satisfied: Pillow>=7.1 in /root/miniconda3/lib/python3.8/site-packages (from detectron2==0.6) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in /root/miniconda3/lib/python3.8/site-packages (from detectron2==0.6) (3.7.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (2.0.7)\n",
      "Requirement already satisfied: termcolor>=1.1 in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (2.3.0)\n",
      "Requirement already satisfied: yacs>=0.1.8 in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (3.0.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /root/miniconda3/lib/python3.8/site-packages (from detectron2==0.6) (4.61.2)\n",
      "Requirement already satisfied: tensorboard in /root/miniconda3/lib/python3.8/site-packages (from detectron2==0.6) (2.12.0)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (0.1.9)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.1 in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (2.3.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (1.3.2)\n",
      "Requirement already satisfied: black in /root/.local/lib/python3.8/site-packages (from detectron2==0.6) (23.11.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.8/site-packages (from detectron2==0.6) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.8/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.8/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.24.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /root/.local/lib/python3.8/site-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
      "Requirement already satisfied: importlib-resources in /root/miniconda3/lib/python3.8/site-packages (from hydra-core>=1.1->detectron2==0.6) (5.12.0)\n",
      "Requirement already satisfied: portalocker in /root/.local/lib/python3.8/site-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (4.39.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /root/miniconda3/lib/python3.8/site-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.6) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->detectron2==0.6) (1.16.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /root/.local/lib/python3.8/site-packages (from black->detectron2==0.6) (0.11.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /root/.local/lib/python3.8/site-packages (from black->detectron2==0.6) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /root/miniconda3/lib/python3.8/site-packages (from black->detectron2==0.6) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /root/miniconda3/lib/python3.8/site-packages (from black->detectron2==0.6) (4.5.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/.local/lib/python3.8/site-packages (from black->detectron2==0.6) (8.1.7)\n",
      "Requirement already satisfied: platformdirs>=2 in /root/miniconda3/lib/python3.8/site-packages (from black->detectron2==0.6) (3.1.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (2.16.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (1.51.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (2.28.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (2.2.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (4.22.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (3.4.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /root/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /root/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /root/miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /root/miniconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (6.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /root/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /root/miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: detectron2 in /root/.local/lib/python3.8/site-packages (0.6)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.1 in /root/.local/lib/python3.8/site-packages (from detectron2) (2.3.0)\n",
      "Requirement already satisfied: tensorboard in /root/miniconda3/lib/python3.8/site-packages (from detectron2) (2.12.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.8/site-packages (from detectron2) (23.0)\n",
      "Requirement already satisfied: tabulate in /root/.local/lib/python3.8/site-packages (from detectron2) (0.9.0)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /root/.local/lib/python3.8/site-packages (from detectron2) (0.1.5.post20221221)\n",
      "Requirement already satisfied: yacs>=0.1.8 in /root/.local/lib/python3.8/site-packages (from detectron2) (0.1.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in /root/.local/lib/python3.8/site-packages (from detectron2) (2.3.0)\n",
      "Requirement already satisfied: black in /root/.local/lib/python3.8/site-packages (from detectron2) (23.11.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /root/miniconda3/lib/python3.8/site-packages (from detectron2) (4.61.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /root/.local/lib/python3.8/site-packages (from detectron2) (2.0.7)\n",
      "Requirement already satisfied: Pillow>=7.1 in /root/miniconda3/lib/python3.8/site-packages (from detectron2) (9.4.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in /root/.local/lib/python3.8/site-packages (from detectron2) (1.3.2)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /root/.local/lib/python3.8/site-packages (from detectron2) (0.1.9)\n",
      "Requirement already satisfied: cloudpickle in /root/.local/lib/python3.8/site-packages (from detectron2) (3.0.0)\n",
      "Requirement already satisfied: matplotlib in /root/miniconda3/lib/python3.8/site-packages (from detectron2) (3.7.1)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.8/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.8/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (6.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /root/.local/lib/python3.8/site-packages (from hydra-core>=1.1->detectron2) (4.9.3)\n",
      "Requirement already satisfied: importlib-resources in /root/miniconda3/lib/python3.8/site-packages (from hydra-core>=1.1->detectron2) (5.12.0)\n",
      "Requirement already satisfied: portalocker in /root/.local/lib/python3.8/site-packages (from iopath<0.1.10,>=0.1.7->detectron2) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.8/site-packages (from matplotlib->detectron2) (4.39.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /root/miniconda3/lib/python3.8/site-packages (from importlib-resources->hydra-core>=1.1->detectron2) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->detectron2) (1.16.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /root/miniconda3/lib/python3.8/site-packages (from black->detectron2) (2.0.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /root/.local/lib/python3.8/site-packages (from black->detectron2) (0.11.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/.local/lib/python3.8/site-packages (from black->detectron2) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /root/miniconda3/lib/python3.8/site-packages (from black->detectron2) (4.5.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /root/.local/lib/python3.8/site-packages (from black->detectron2) (1.0.0)\n",
      "Requirement already satisfied: platformdirs>=2 in /root/miniconda3/lib/python3.8/site-packages (from black->detectron2) (3.1.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (1.51.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (2.16.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (2.2.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (4.22.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (0.7.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (1.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (2.28.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /root/miniconda3/lib/python3.8/site-packages (from tensorboard->detectron2) (3.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /root/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /root/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /root/miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /root/miniconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->detectron2) (6.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /root/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /root/miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# @formatter:off\n",
    "# https://pytorch.org/get-started/previous-versions/\n",
    "!pip3 install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu118 --user\n",
    "!pip install 'git+https://github.com/facebookresearch/detectron2.git' --user\n",
    "!pip install detectron2\n",
    "# @formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spoken-deputy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu118 True\n",
      "0.15.1+cu118\n",
      "gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
      "Copyright (C) 2019 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "print(torchvision.__version__)\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "solved-engineering",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc090d0717a0aa6",
   "metadata": {},
   "source": [
    "## Collect Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b163b83-b2cf-4d84-9f89-18825c213bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据增强\n",
    "from detectron2.data import transforms as T\n",
    "\n",
    "def custom_augmentation():\n",
    "    # 创建一个增强列表\n",
    "    augmentation_list = [\n",
    "        T.RandomFlip(prob=0.5, horizontal=True, vertical=False),  # 随机水平翻转\n",
    "        T.RandomRotation(angle=[0, 90, 180, 270], sample_style='choice'),  # 随机旋转\n",
    "        T.RandomExtent(scale_range=(0.8, 1.2), shift_range=(0.0, 0.0)),  # 随机扩展\n",
    "        T.ResizeShortestEdge(short_edge_length=[640, 672, 704, 736, 768, 800], max_size=1333, sample_style='choice')  # 随机调整大小\n",
    "    ]\n",
    "    return T.AugmentationList(augmentation_list)\n",
    "\n",
    "from detectron2.data import build_detection_train_loader\n",
    "\n",
    "def mapper(dataset_dict):\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # 它是防止对原始数据集造成影响\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "    transform_list = custom_augmentation()\n",
    "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "        obj_transform(a, transforms)\n",
    "        for a in dataset_dict.pop(\"annotations\")\n",
    "        if obj_transform(a, transforms) is not None\n",
    "    ]\n",
    "    dataset_dict[\"annotations\"] = annos\n",
    "    return dataset_dict\n",
    "\n",
    "def custom_train_loader(cfg):\n",
    "    return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f56b0c57c09a267",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes [\n",
      "  \"arrow\",\n",
      "  \"connection\",\n",
      "  \"data\",\n",
      "  \"decision\",\n",
      "  \"process\",\n",
      "  \"terminator\",\n",
      "  \"text\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from src.utils.utils_json import print_json, read_json\n",
    "from src.dataset.dataset import read_dateset_from\n",
    "\n",
    "# @formatter:off\n",
    "# datadir = \"datasets/fa\"\n",
    "datadir = \"datasets/fcb\"\n",
    "# datadir = \"Sketches-Dataset-main/data\"\n",
    "# @formatter:on\n",
    "\n",
    "training_meta_path = os.path.join(datadir, \"train.json\")\n",
    "classes = list(map(lambda x: x[\"name\"], read_json(training_meta_path)[\"categories\"]))\n",
    "classes.sort()\n",
    "\n",
    "print_json(classes, tag=\"Classes\")\n",
    "\n",
    "\n",
    "def on_register_dataset(x):\n",
    "    dataset = read_dateset_from(datadir, x)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "for d in [\"train\", \"val\"]:\n",
    "    name = \"sketches_\" + d\n",
    "\n",
    "    # Remove previously registered datasets if they exist\n",
    "    try:\n",
    "        DatasetCatalog.remove(name)\n",
    "        MetadataCatalog.remove(name)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Register new dataset\n",
    "    DatasetCatalog.register(name, lambda x=d: on_register_dataset(x))\n",
    "    MetadataCatalog.get(name).set(thing_classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "407a1424137ddbc3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "sketches_metadata = MetadataCatalog.get(\"sketches_train\")\n",
    "training_dataset = read_dateset_from(datadir, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907872d3c0f4cece",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thing_classes [\n",
      "  \"arrow\",\n",
      "  \"connection\",\n",
      "  \"data\",\n",
      "  \"decision\",\n",
      "  \"process\",\n",
      "  \"terminator\",\n",
      "  \"text\"\n",
      "]\n",
      "keypoint_names [\n",
      "  \"head\",\n",
      "  \"tail\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#metadata-for-datasets\n",
    "MetadataCatalog.get(\"sketches_train\").keypoint_names = [\n",
    "    \"head\",\n",
    "    \"tail\",\n",
    "]\n",
    "\n",
    "MetadataCatalog.get(\"sketches_train\").keypoint_flip_map = (\n",
    "    (\"head\", \"tail\"),\n",
    "    (\"tail\", \"head\"),\n",
    ")\n",
    "\n",
    "print_json(MetadataCatalog.get(\"sketches_train\").thing_classes, tag=\"thing_classes\")\n",
    "print_json(MetadataCatalog.get(\"sketches_train\").keypoint_names, tag=\"keypoint_names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22fc9f93fed232",
   "metadata": {},
   "source": [
    "## Configure Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "103f5463e0007527",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: 'fcb_7'\n"
     ]
    }
   ],
   "source": [
    "from names_generator import generate_name\n",
    "\n",
    "model_name = \"fcb_7\"\n",
    "print(f\"Model name: '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fitting-philosophy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"CUDNN_BENCHMARK\": false,\n",
      "  \"DATALOADER\": {\n",
      "    \"ASPECT_RATIO_GROUPING\": true,\n",
      "    \"FILTER_EMPTY_ANNOTATIONS\": true,\n",
      "    \"NUM_WORKERS\": 48,\n",
      "    \"REPEAT_THRESHOLD\": 0.0,\n",
      "    \"SAMPLER_TRAIN\": \"TrainingSampler\"\n",
      "  },\n",
      "  \"DATASETS\": {\n",
      "    \"PRECOMPUTED_PROPOSAL_TOPK_TEST\": 1000,\n",
      "    \"PRECOMPUTED_PROPOSAL_TOPK_TRAIN\": 2000,\n",
      "    \"PROPOSAL_FILES_TEST\": [],\n",
      "    \"PROPOSAL_FILES_TRAIN\": [],\n",
      "    \"TEST\": [\n",
      "      \"sketches_val\"\n",
      "    ],\n",
      "    \"TRAIN\": [\n",
      "      \"sketches_train\"\n",
      "    ]\n",
      "  },\n",
      "  \"GLOBAL\": {\n",
      "    \"HACK\": 1.0\n",
      "  },\n",
      "  \"INPUT\": {\n",
      "    \"CROP\": {\n",
      "      \"ENABLED\": false,\n",
      "      \"SIZE\": [\n",
      "        0.9,\n",
      "        0.9\n",
      "      ],\n",
      "      \"TYPE\": \"relative_range\"\n",
      "    },\n",
      "    \"FORMAT\": \"BGR\",\n",
      "    \"MASK_FORMAT\": \"polygon\",\n",
      "    \"MAX_SIZE_TEST\": 1333,\n",
      "    \"MAX_SIZE_TRAIN\": 1333,\n",
      "    \"MIN_SIZE_TEST\": 800,\n",
      "    \"MIN_SIZE_TRAIN\": [\n",
      "      640,\n",
      "      672,\n",
      "      704,\n",
      "      736,\n",
      "      768,\n",
      "      800\n",
      "    ],\n",
      "    \"MIN_SIZE_TRAIN_SAMPLING\": \"choice\",\n",
      "    \"RANDOM_FLIP\": \"horizontal\"\n",
      "  },\n",
      "  \"MODEL\": {\n",
      "    \"ANCHOR_GENERATOR\": {\n",
      "      \"ANGLES\": [\n",
      "        [\n",
      "          -90,\n",
      "          0,\n",
      "          90\n",
      "        ]\n",
      "      ],\n",
      "      \"ASPECT_RATIOS\": [\n",
      "        [\n",
      "          0.5,\n",
      "          1.0,\n",
      "          2.0\n",
      "        ]\n",
      "      ],\n",
      "      \"NAME\": \"DefaultAnchorGenerator\",\n",
      "      \"OFFSET\": 0.0,\n",
      "      \"SIZES\": [\n",
      "        [\n",
      "          32\n",
      "        ],\n",
      "        [\n",
      "          64\n",
      "        ],\n",
      "        [\n",
      "          128\n",
      "        ],\n",
      "        [\n",
      "          256\n",
      "        ],\n",
      "        [\n",
      "          512\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"BACKBONE\": {\n",
      "      \"FREEZE_AT\": 0,\n",
      "      \"NAME\": \"build_resnet_fpn_backbone\"\n",
      "    },\n",
      "    \"DEVICE\": \"cuda\",\n",
      "    \"FPN\": {\n",
      "      \"FUSE_TYPE\": \"sum\",\n",
      "      \"IN_FEATURES\": [\n",
      "        \"res2\",\n",
      "        \"res3\",\n",
      "        \"res4\",\n",
      "        \"res5\"\n",
      "      ],\n",
      "      \"NORM\": \"\",\n",
      "      \"OUT_CHANNELS\": 256\n",
      "    },\n",
      "    \"KEYPOINT_ON\": true,\n",
      "    \"LOAD_PROPOSALS\": false,\n",
      "    \"MASK_ON\": false,\n",
      "    \"META_ARCHITECTURE\": \"GeneralizedRCNN\",\n",
      "    \"PANOPTIC_FPN\": {\n",
      "      \"COMBINE\": {\n",
      "        \"ENABLED\": true,\n",
      "        \"INSTANCES_CONFIDENCE_THRESH\": 0.5,\n",
      "        \"OVERLAP_THRESH\": 0.5,\n",
      "        \"STUFF_AREA_LIMIT\": 4096\n",
      "      },\n",
      "      \"INSTANCE_LOSS_WEIGHT\": 1.0\n",
      "    },\n",
      "    \"PIXEL_MEAN\": [\n",
      "      103.53,\n",
      "      116.28,\n",
      "      123.675\n",
      "    ],\n",
      "    \"PIXEL_STD\": [\n",
      "      57.375,\n",
      "      57.12,\n",
      "      58.395\n",
      "    ],\n",
      "    \"PROPOSAL_GENERATOR\": {\n",
      "      \"MIN_SIZE\": 0,\n",
      "      \"NAME\": \"RPN\"\n",
      "    },\n",
      "    \"RESNETS\": {\n",
      "      \"DEFORM_MODULATED\": false,\n",
      "      \"DEFORM_NUM_GROUPS\": 1,\n",
      "      \"DEFORM_ON_PER_STAGE\": [\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false\n",
      "      ],\n",
      "      \"DEPTH\": 101,\n",
      "      \"NORM\": \"FrozenBN\",\n",
      "      \"NUM_GROUPS\": 32,\n",
      "      \"OUT_FEATURES\": [\n",
      "        \"res2\",\n",
      "        \"res3\",\n",
      "        \"res4\",\n",
      "        \"res5\"\n",
      "      ],\n",
      "      \"RES2_OUT_CHANNELS\": 256,\n",
      "      \"RES5_DILATION\": 1,\n",
      "      \"STEM_OUT_CHANNELS\": 64,\n",
      "      \"STRIDE_IN_1X1\": false,\n",
      "      \"WIDTH_PER_GROUP\": 8\n",
      "    },\n",
      "    \"RETINANET\": {\n",
      "      \"BBOX_REG_LOSS_TYPE\": \"smooth_l1\",\n",
      "      \"BBOX_REG_WEIGHTS\": [\n",
      "        1.0,\n",
      "        1.0,\n",
      "        1.0,\n",
      "        1.0\n",
      "      ],\n",
      "      \"FOCAL_LOSS_ALPHA\": 0.25,\n",
      "      \"FOCAL_LOSS_GAMMA\": 2.0,\n",
      "      \"IN_FEATURES\": [\n",
      "        \"p3\",\n",
      "        \"p4\",\n",
      "        \"p5\",\n",
      "        \"p6\",\n",
      "        \"p7\"\n",
      "      ],\n",
      "      \"IOU_LABELS\": [\n",
      "        0,\n",
      "        -1,\n",
      "        1\n",
      "      ],\n",
      "      \"IOU_THRESHOLDS\": [\n",
      "        0.4,\n",
      "        0.5\n",
      "      ],\n",
      "      \"NMS_THRESH_TEST\": 0.5,\n",
      "      \"NORM\": \"\",\n",
      "      \"NUM_CLASSES\": 80,\n",
      "      \"NUM_CONVS\": 4,\n",
      "      \"PRIOR_PROB\": 0.01,\n",
      "      \"SCORE_THRESH_TEST\": 0.05,\n",
      "      \"SMOOTH_L1_LOSS_BETA\": 0.1,\n",
      "      \"TOPK_CANDIDATES_TEST\": 1000\n",
      "    },\n",
      "    \"ROI_BOX_CASCADE_HEAD\": {\n",
      "      \"BBOX_REG_WEIGHTS\": [\n",
      "        [\n",
      "          10.0,\n",
      "          10.0,\n",
      "          5.0,\n",
      "          5.0\n",
      "        ],\n",
      "        [\n",
      "          20.0,\n",
      "          20.0,\n",
      "          10.0,\n",
      "          10.0\n",
      "        ],\n",
      "        [\n",
      "          30.0,\n",
      "          30.0,\n",
      "          15.0,\n",
      "          15.0\n",
      "        ]\n",
      "      ],\n",
      "      \"IOUS\": [\n",
      "        0.5,\n",
      "        0.6,\n",
      "        0.7\n",
      "      ]\n",
      "    },\n",
      "    \"ROI_BOX_HEAD\": {\n",
      "      \"BBOX_REG_LOSS_TYPE\": \"smooth_l1\",\n",
      "      \"BBOX_REG_LOSS_WEIGHT\": 1.0,\n",
      "      \"BBOX_REG_WEIGHTS\": [\n",
      "        10.0,\n",
      "        10.0,\n",
      "        5.0,\n",
      "        5.0\n",
      "      ],\n",
      "      \"CLS_AGNOSTIC_BBOX_REG\": false,\n",
      "      \"CONV_DIM\": 256,\n",
      "      \"FC_DIM\": 1024,\n",
      "      \"FED_LOSS_FREQ_WEIGHT_POWER\": 0.5,\n",
      "      \"FED_LOSS_NUM_CLASSES\": 50,\n",
      "      \"NAME\": \"FastRCNNConvFCHead\",\n",
      "      \"NORM\": \"\",\n",
      "      \"NUM_CONV\": 0,\n",
      "      \"NUM_FC\": 2,\n",
      "      \"POOLER_RESOLUTION\": 7,\n",
      "      \"POOLER_SAMPLING_RATIO\": 0,\n",
      "      \"POOLER_TYPE\": \"ROIAlignV2\",\n",
      "      \"SMOOTH_L1_BETA\": 0.0,\n",
      "      \"TRAIN_ON_PRED_BOXES\": false,\n",
      "      \"USE_FED_LOSS\": false,\n",
      "      \"USE_SIGMOID_CE\": false\n",
      "    },\n",
      "    \"ROI_HEADS\": {\n",
      "      \"BATCH_SIZE_PER_IMAGE\": 128,\n",
      "      \"IN_FEATURES\": [\n",
      "        \"p2\",\n",
      "        \"p3\",\n",
      "        \"p4\",\n",
      "        \"p5\"\n",
      "      ],\n",
      "      \"IOU_LABELS\": [\n",
      "        0,\n",
      "        1\n",
      "      ],\n",
      "      \"IOU_THRESHOLDS\": [\n",
      "        0.5\n",
      "      ],\n",
      "      \"NAME\": \"SketchROIHeads\",\n",
      "      \"NMS_THRESH_TEST\": 0.5,\n",
      "      \"NUM_CLASSES\": 7,\n",
      "      \"POSITIVE_FRACTION\": 0.25,\n",
      "      \"PROPOSAL_APPEND_GT\": true,\n",
      "      \"SCORE_THRESH_TEST\": 0.05\n",
      "    },\n",
      "    \"ROI_KEYPOINT_HEAD\": {\n",
      "      \"CONV_DIMS\": [\n",
      "        512,\n",
      "        512,\n",
      "        512,\n",
      "        512,\n",
      "        512,\n",
      "        512,\n",
      "        512,\n",
      "        512\n",
      "      ],\n",
      "      \"LOSS_WEIGHT\": 1.0,\n",
      "      \"MIN_KEYPOINTS_PER_IMAGE\": 1,\n",
      "      \"NAME\": \"KRCNNConvDeconvUpsampleHead\",\n",
      "      \"NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\": true,\n",
      "      \"NUM_KEYPOINTS\": 2,\n",
      "      \"POOLER_RESOLUTION\": 14,\n",
      "      \"POOLER_SAMPLING_RATIO\": 0,\n",
      "      \"POOLER_TYPE\": \"ROIAlignV2\"\n",
      "    },\n",
      "    \"ROI_MASK_HEAD\": {\n",
      "      \"CLS_AGNOSTIC_MASK\": false,\n",
      "      \"CONV_DIM\": 256,\n",
      "      \"NAME\": \"MaskRCNNConvUpsampleHead\",\n",
      "      \"NORM\": \"\",\n",
      "      \"NUM_CONV\": 4,\n",
      "      \"POOLER_RESOLUTION\": 14,\n",
      "      \"POOLER_SAMPLING_RATIO\": 0,\n",
      "      \"POOLER_TYPE\": \"ROIAlignV2\"\n",
      "    },\n",
      "    \"RPN\": {\n",
      "      \"BATCH_SIZE_PER_IMAGE\": 256,\n",
      "      \"BBOX_REG_LOSS_TYPE\": \"smooth_l1\",\n",
      "      \"BBOX_REG_LOSS_WEIGHT\": 1.0,\n",
      "      \"BBOX_REG_WEIGHTS\": [\n",
      "        1.0,\n",
      "        1.0,\n",
      "        1.0,\n",
      "        1.0\n",
      "      ],\n",
      "      \"BOUNDARY_THRESH\": -1,\n",
      "      \"CONV_DIMS\": [\n",
      "        -1\n",
      "      ],\n",
      "      \"HEAD_NAME\": \"StandardRPNHead\",\n",
      "      \"IN_FEATURES\": [\n",
      "        \"p2\",\n",
      "        \"p3\",\n",
      "        \"p4\",\n",
      "        \"p5\",\n",
      "        \"p6\"\n",
      "      ],\n",
      "      \"IOU_LABELS\": [\n",
      "        0,\n",
      "        -1,\n",
      "        1\n",
      "      ],\n",
      "      \"IOU_THRESHOLDS\": [\n",
      "        0.3,\n",
      "        0.7\n",
      "      ],\n",
      "      \"LOSS_WEIGHT\": 1.0,\n",
      "      \"NMS_THRESH\": 0.7,\n",
      "      \"POSITIVE_FRACTION\": 0.5,\n",
      "      \"POST_NMS_TOPK_TEST\": 1000,\n",
      "      \"POST_NMS_TOPK_TRAIN\": 1000,\n",
      "      \"PRE_NMS_TOPK_TEST\": 1000,\n",
      "      \"PRE_NMS_TOPK_TRAIN\": 2000,\n",
      "      \"SMOOTH_L1_BETA\": 0.0\n",
      "    },\n",
      "    \"SEM_SEG_HEAD\": {\n",
      "      \"COMMON_STRIDE\": 4,\n",
      "      \"CONVS_DIM\": 128,\n",
      "      \"IGNORE_VALUE\": 255,\n",
      "      \"IN_FEATURES\": [\n",
      "        \"p2\",\n",
      "        \"p3\",\n",
      "        \"p4\",\n",
      "        \"p5\"\n",
      "      ],\n",
      "      \"LOSS_WEIGHT\": 1.0,\n",
      "      \"NAME\": \"SemSegFPNHead\",\n",
      "      \"NORM\": \"GN\",\n",
      "      \"NUM_CLASSES\": 54\n",
      "    },\n",
      "    \"WEIGHTS\": \"/root/sourcecode//lissa-diagram-recognition/notebooks/models/fcb_6/model_final.pth\"\n",
      "  },\n",
      "  \"OUTPUT_DIR\": \"models/fcb_7\",\n",
      "  \"SEED\": -1,\n",
      "  \"SOLVER\": {\n",
      "    \"AMP\": {\n",
      "      \"ENABLED\": false\n",
      "    },\n",
      "    \"BASE_LR\": 0.0025,\n",
      "    \"BASE_LR_END\": 0.0,\n",
      "    \"BIAS_LR_FACTOR\": 1.0,\n",
      "    \"CHECKPOINT_PERIOD\": 5000,\n",
      "    \"CLIP_GRADIENTS\": {\n",
      "      \"CLIP_TYPE\": \"value\",\n",
      "      \"CLIP_VALUE\": 1.0,\n",
      "      \"ENABLED\": false,\n",
      "      \"NORM_TYPE\": 2.0\n",
      "    },\n",
      "    \"GAMMA\": 0.1,\n",
      "    \"IMS_PER_BATCH\": 2,\n",
      "    \"LR_SCHEDULER_NAME\": \"WarmupMultiStepLR\",\n",
      "    \"MAX_ITER\": 45000,\n",
      "    \"MOMENTUM\": 0.9,\n",
      "    \"NESTEROV\": false,\n",
      "    \"NUM_DECAYS\": 3,\n",
      "    \"REFERENCE_WORLD_SIZE\": 0,\n",
      "    \"RESCALE_INTERVAL\": false,\n",
      "    \"STEPS\": [\n",
      "      30000,\n",
      "      40000\n",
      "    ],\n",
      "    \"WARMUP_FACTOR\": 0.001,\n",
      "    \"WARMUP_ITERS\": 1000,\n",
      "    \"WARMUP_METHOD\": \"linear\",\n",
      "    \"WEIGHT_DECAY\": 0.0001,\n",
      "    \"WEIGHT_DECAY_BIAS\": null,\n",
      "    \"WEIGHT_DECAY_NORM\": 0.0\n",
      "  },\n",
      "  \"TEST\": {\n",
      "    \"AUG\": {\n",
      "      \"ENABLED\": false,\n",
      "      \"FLIP\": true,\n",
      "      \"MAX_SIZE\": 4000,\n",
      "      \"MIN_SIZES\": [\n",
      "        400,\n",
      "        500,\n",
      "        600,\n",
      "        700,\n",
      "        800,\n",
      "        900,\n",
      "        1000,\n",
      "        1100,\n",
      "        1200\n",
      "      ]\n",
      "    },\n",
      "    \"DETECTIONS_PER_IMAGE\": 100,\n",
      "    \"EVAL_PERIOD\": 0,\n",
      "    \"EXPECTED_RESULTS\": [],\n",
      "    \"KEYPOINT_OKS_SIGMAS\": [\n",
      "      1.0,\n",
      "      1.0\n",
      "    ],\n",
      "    \"PRECISE_BN\": {\n",
      "      \"ENABLED\": false,\n",
      "      \"NUM_ITER\": 200\n",
      "    }\n",
      "  },\n",
      "  \"VERSION\": 2,\n",
      "  \"VIS_PERIOD\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "from src.utils.utils_json import write_json\n",
    "from src.sketch_detection_rcnn.roi_heads import SketchROIHeads  # noqa # pylint: disable=unused-import\n",
    "\n",
    "pretrained_model = \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"\n",
    "\n",
    "cfg = get_cfg()\n",
    "\n",
    "# Use pre-trained model\n",
    "cfg.merge_from_file(model_zoo.get_config_file(pretrained_model))\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(pretrained_model) sourcecode/lissa-diagram-recognition/notebooks/models/fcb_2/model_final.pth\n",
    "cfg.MODEL.WEIGHTS = \"/root/sourcecode//lissa-diagram-recognition/notebooks/models/fcb_6/model_final.pth\"\n",
    "# Use pre-pre-trained model\n",
    "# cfg.merge_from_file(model_zoo.get_config_file(pretrained_model))\n",
    "# cfg.MODEL.WEIGHTS = \"/home/jupyter-patrickzierahn/models/clever_mahavira/model_final.pth\"\n",
    "\n",
    "cfg.OUTPUT_DIR = os.path.join(\"models\", model_name)\n",
    "\n",
    "# Set training data\n",
    "cfg.DATALOADER.NUM_WORKERS = multiprocessing.cpu_count()\n",
    "cfg.DATASETS.TRAIN = (\"sketches_train\",)\n",
    "cfg.DATASETS.TEST = (\"sketches_val\",)\n",
    "\n",
    "# Set the backbone to be trainable\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT = 0\n",
    "\n",
    "# pick a good LR\n",
    "# 设置动量\n",
    "cfg.SOLVER.MOMENTUM = 0.9\n",
    "\n",
    "# 设置权重衰减\n",
    "cfg.SOLVER.WEIGHT_DECAY = 0.0001\n",
    "cfg.SOLVER.GAMMA = 0.1\n",
    "cfg.SOLVER.BASE_LR = 0.0025\n",
    "cfg.SOLVER.STEPS = (30000, 40000)\n",
    "cfg.SOLVER.MAX_ITER = 45000\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "\n",
    "# 设置loss权重\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_WEIGHT = 1\n",
    "cfg.MODEL.ROI_BOX_HEAD.CLS_LOSS_WEIGHT = 1\n",
    "cfg.MODEL.ROI_BOX_HEAD.ARROW_REG_LOSS_WEIGHT = 5\n",
    "\n",
    "# Region of Interest\n",
    "cfg.MODEL.ROI_HEADS.NAME = \"SketchROIHeads\"\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)\n",
    "\n",
    "# Keypoints\n",
    "cfg.MODEL.KEYPOINT_ON = True\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 2\n",
    "cfg.TEST.KEYPOINT_OKS_SIGMAS = [1.0] * cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS\n",
    "\n",
    "# Write config to file\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "cfg_file = os.path.join(cfg.OUTPUT_DIR, \"cfg.json\")\n",
    "write_json(cfg_file, cfg)\n",
    "\n",
    "# Write class names to file\n",
    "classes_file = os.path.join(cfg.OUTPUT_DIR, \"classes.json\")\n",
    "write_json(classes_file, classes)\n",
    "\n",
    "# Print config\n",
    "print_json(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bb01f0da489a3",
   "metadata": {},
   "source": [
    "## Print Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1327fa7a87f6c6eb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_model\n",
    "\n",
    "model = build_model(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1615c9786b88c1",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea988a8fa5d9f3e",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/28 02:14:32 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): SketchROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): SketchRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=8, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=28, bias=True)\n",
      "      (arrow_keypoint_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[11/28 02:14:33 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 280 images left.\n",
      "\u001b[32m[11/28 02:14:33 d2.data.build]: \u001b[0mRemoved 0 images with fewer than 1 keypoints.\n",
      "\u001b[32m[11/28 02:14:33 d2.data.build]: \u001b[0mDistribution of instances among all 7 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|   arrow    | 158          | connection | 509          |    data    | 320          |\n",
      "|  decision  | 541          |  process   | 379          | terminator | 2388         |\n",
      "|    text    | 1900         |            |              |            |              |\n",
      "|   total    | 6195         |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[11/28 02:14:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[11/28 02:14:33 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[11/28 02:14:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[11/28 02:14:33 d2.data.common]: \u001b[0mSerializing 280 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/28 02:14:33 d2.data.common]: \u001b[0mSerialized dataset takes 1.23 MiB\n",
      "\u001b[32m[11/28 02:14:33 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
      "\u001b[32m[11/28 02:14:33 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /root/sourcecode//lissa-diagram-recognition/notebooks/models/fcb_6/model_final.pth ...\n",
      "\u001b[32m[11/28 02:14:33 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/.local/lib/python3.8/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/28 02:14:49 d2.utils.events]: \u001b[0m eta: 6:50:57  iter: 19  total_loss: 0.07904  loss_cls: 0.02102  loss_box_reg: 0.0325  loss_arrow_reg: 0.01333  loss_rpn_cls: 0.0001255  loss_rpn_loc: 0.01056    time: 0.5515  last_time: 0.5964  data_time: 0.2090  last_data_time: 0.0147   lr: 4.9952e-05  max_mem: 7403M\n",
      "\u001b[32m[11/28 02:15:00 d2.utils.events]: \u001b[0m eta: 6:43:47  iter: 39  total_loss: 0.07688  loss_cls: 0.02019  loss_box_reg: 0.033  loss_arrow_reg: 0.01191  loss_rpn_cls: 0.0001383  loss_rpn_loc: 0.01052    time: 0.5244  last_time: 0.5206  data_time: 0.0015  last_data_time: 0.0010   lr: 9.9902e-05  max_mem: 7403M\n",
      "\u001b[32m[11/28 02:15:10 d2.utils.events]: \u001b[0m eta: 6:34:01  iter: 59  total_loss: 0.09164  loss_cls: 0.02371  loss_box_reg: 0.0361  loss_arrow_reg: 0.01601  loss_rpn_cls: 0.0001358  loss_rpn_loc: 0.01059    time: 0.5199  last_time: 0.4707  data_time: 0.0038  last_data_time: 0.0009   lr: 0.00014985  max_mem: 7403M\n",
      "\u001b[32m[11/28 02:15:20 d2.utils.events]: \u001b[0m eta: 6:30:30  iter: 79  total_loss: 0.08182  loss_cls: 0.01715  loss_box_reg: 0.03448  loss_arrow_reg: 0.01164  loss_rpn_cls: 8.859e-05  loss_rpn_loc: 0.01144    time: 0.5116  last_time: 0.5361  data_time: 0.0051  last_data_time: 0.0064   lr: 0.0001998  max_mem: 7403M\n",
      "\u001b[32m[11/28 02:15:29 d2.utils.events]: \u001b[0m eta: 6:22:05  iter: 99  total_loss: 0.1005  loss_cls: 0.02851  loss_box_reg: 0.03711  loss_arrow_reg: 0.01388  loss_rpn_cls: 0.000118  loss_rpn_loc: 0.01396    time: 0.5048  last_time: 0.3903  data_time: 0.0050  last_data_time: 0.0023   lr: 0.00024975  max_mem: 7403M\n",
      "\u001b[32m[11/28 02:15:39 d2.utils.events]: \u001b[0m eta: 6:19:59  iter: 119  total_loss: 0.09046  loss_cls: 0.01903  loss_box_reg: 0.03707  loss_arrow_reg: 0.01458  loss_rpn_cls: 0.0001451  loss_rpn_loc: 0.01534    time: 0.5025  last_time: 0.4885  data_time: 0.0049  last_data_time: 0.0046   lr: 0.0002997  max_mem: 7403M\n",
      "\u001b[32m[11/28 02:15:48 d2.utils.events]: \u001b[0m eta: 6:15:18  iter: 139  total_loss: 0.09788  loss_cls: 0.02772  loss_box_reg: 0.03971  loss_arrow_reg: 0.01463  loss_rpn_cls: 0.0001681  loss_rpn_loc: 0.01417    time: 0.4983  last_time: 0.5133  data_time: 0.0046  last_data_time: 0.0054   lr: 0.00034965  max_mem: 7403M\n",
      "\u001b[32m[11/28 02:15:58 d2.utils.events]: \u001b[0m eta: 6:14:52  iter: 159  total_loss: 0.1066  loss_cls: 0.02292  loss_box_reg: 0.04177  loss_arrow_reg: 0.02032  loss_rpn_cls: 0.0001157  loss_rpn_loc: 0.01636    time: 0.4964  last_time: 0.4705  data_time: 0.0050  last_data_time: 0.0055   lr: 0.0003996  max_mem: 7403M\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return custom_train_loader(cfg)\n",
    "    \n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "training_duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19f2fe68770d4d",
   "metadata": {},
   "source": [
    "## Gather Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44406a4e32a824",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# path to the model we just trained\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4  # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacfb0b787d622f",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from src.visualization.visualize import show_img\n",
    "\n",
    "validation_dataset = read_dateset_from(datadir, \"val\")\n",
    "\n",
    "for record in random.sample(validation_dataset, 1):\n",
    "    im = cv2.imread(record[\"file_name\"])\n",
    "    print(record[\"file_name\"])\n",
    "    # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    outputs = predictor(im)\n",
    "\n",
    "    # print_json(record, tag=\"record\")\n",
    "    # print(\"outputs\", outputs)\n",
    "\n",
    "    # instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    v = Visualizer(im, metadata=sketches_metadata, scale=0.5)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    show_img(out.get_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c9758e8c63641",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "outdir = os.path.join(\"reports\", model_name)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "test_datasets = cfg.DATASETS.TEST\n",
    "evaluator = [\n",
    "    COCOEvaluator(\n",
    "        test_set,\n",
    "        cfg,\n",
    "        distributed=False,\n",
    "        output_dir=outdir,\n",
    "    )\n",
    "    for test_set in test_datasets\n",
    "]\n",
    "\n",
    "metrics = DefaultTrainer.test(cfg, predictor.model, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e0a648eb5e898",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convert inference to a dict and remove NaN values\n",
    "inference_dict = dict((k, v) for k, v in metrics.items())\n",
    "metrics_file = os.path.join(cfg.OUTPUT_DIR, \"metrics.json\")\n",
    "\n",
    "report = {\n",
    "    \"model_name\": model_name,\n",
    "    \"training_time\": training_duration,\n",
    "    \"training_data\": datadir,\n",
    "    \"model_path\": cfg.OUTPUT_DIR,\n",
    "    \"config_file\": cfg_file,\n",
    "    \"inference\": inference_dict,\n",
    "    \"metrics_file\": metrics_file,\n",
    "    \"date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "\n",
    "report_file = f\"reports/{model_name}.json\"\n",
    "write_json(report_file, report)\n",
    "\n",
    "print_json(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372d56286c9749d",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Model name: '{model_name}', training time: {training_duration / 60:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6d895-a165-4a37-85cc-2a46b7765cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
